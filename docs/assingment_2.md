
# CSML25 ‚Äì Assignment 2: Machine learning with text data

Trang n√†y t√≥m t·∫Øt k·∫øt qu·∫£ t·ª´ notebook `CSML25_BTL2.ipynb` trong m√¥n Machine Learning (CO3117, HK 251).

## 1. B√†i to√°n & d·ªØ li·ªáu

**M·ª•c ti√™u**

- X√¢y d·ª±ng pipeline machine learning truy·ªÅn th·ªëng cho b√†i to√°n ph√¢n lo·∫°i c·∫£m x√∫c t·ª´ c√¢u ti·∫øng Anh.
- Th·ª±c hi·ªán EDA: th·ªëng k√™ ƒë·ªô d√†i c√¢u, ph√¢n b·ªë nh√£n, t·∫ßn su·∫•t t·ª´.
- So s√°nh c√°c m√¥ h√¨nh truy·ªÅn th·ªëng (BoW, TF-IDF + LR / NB / SVM).
- X√¢y d·ª±ng pipeline deep learning d√πng embedding t·ª´ CNN + pretrained word embeddings.

**Dataset**

- Ngu·ªìn: Kaggle ‚Äì *Emotions dataset for NLP* (`praveengovi/emotions-dataset-for-nlp`).
- Task: ph√¢n lo·∫°i c√¢u ti·∫øng Anh v√†o 6 c·∫£m x√∫c: `joy`, `sadness`, `anger`, `fear`, `love`, `surprise`.
- K√≠ch th∆∞·ªõc:
  - Train: 16 000 m·∫´u  
  - Validation: 2 000 m·∫´u  
  - Test: 2 000 m·∫´u  

**Ph√¢n b·ªë nh√£n (train)**

| Emotion  | S·ªë m·∫´u |
|----------|--------|
| joy      | 5 362  |
| sadness  | 4 666  |
| anger    | 2 159  |
| fear     | 1 937  |
| love     | 1 304  |
| surprise |   572  |

Nh·∫≠n x√©t: d·ªØ li·ªáu kh√° m·∫•t c√¢n b·∫±ng, hai l·ªõp `joy` v√† `sadness` chi·∫øm ƒëa s·ªë, trong khi `love` v√† ƒë·∫∑c bi·ªát `surprise` r·∫•t √≠t.

## 2. Kh√°m ph√° d·ªØ li·ªáu (EDA)

M·ªôt s·ªë th·ªëng k√™ v·ªÅ ƒë·ªô d√†i c√¢u (s·ªë t·ª´ / c√¢u) tr√™n t·∫≠p train:

| Th·ªëng k√™          | Gi√° tr·ªã  |
|-------------------|----------|
| S·ªë m·∫´u            | 16 000   |
| ƒê·ªô d√†i trung b√¨nh | ‚âà 19.17 t·ª´ |
| ƒê·ªô l·ªách chu·∫©n     | ‚âà 10.99  |
| Min / Max         | 2 / 66   |
| Q1 / Q2 / Q3      | 11 / 17 / 25 |

ƒê·ªô d√†i trung b√¨nh theo t·ª´ng c·∫£m x√∫c (train):

- `love`: ~20.70 t·ª´  
- `surprise`: ~19.97 t·ª´  
- `joy`: ~19.50 t·ª´  
- `anger`: ~19.23 t·ª´  
- `fear`: ~18.84 t·ª´  
- `sadness`: ~18.36 t·ª´  

Ngo√†i ra, notebook c√≤n:

- V·∫Ω pie chart v√† bar chart cho ph√¢n b·ªë nh√£n.  
- Th·ªëng k√™ top-20 t·ª´ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t v√† tr·ª±c quan h√≥a word frequency.  
- V·∫Ω boxplot ƒë·ªô d√†i c√¢u theo t·ª´ng c·∫£m x√∫c.  
- Ki·ªÉm tra tr√πng l·∫∑p:
  - Train c√≥ 1 d√≤ng tr√πng l·∫∑p, Validation/Test kh√¥ng c√≥ d√≤ng tr√πng.

## 3. Pipeline truy·ªÅn th·ªëng (BoW / TF-IDF)

### 3.1 Ti·ªÅn x·ª≠ l√Ω & ƒë·∫∑c tr∆∞ng

- Kh√¥ng √°p d·ª•ng ti·ªÅn x·ª≠ l√Ω ph·ª©c t·∫°p (gi·ªØ nguy√™n c√¢u g·ªëc).  
- Bi·ªÉu di·ªÖn vƒÉn b·∫£n:
  - Bag-of-Words (BoW) v·ªõi n-gram (1‚Äì2, bigram).  
  - TF-IDF v·ªõi n-gram (unigram, bigram, trigram).  
- Mapping nh√£n c·∫£m x√∫c ‚Üí nh√£n s·ªë `label` ƒë·ªÉ hu·∫•n luy·ªán model.

### 3.2 C√°c m√¥ h√¨nh ƒë√£ th·ª≠

Nh√≥m th·ª±c nghi·ªám nhi·ªÅu c·∫•u h√¨nh, c√≥ th·ªÉ gom v√†o c√°c nh√≥m ch√≠nh:

- **Baseline (kh√¥ng tuning, kh√¥ng balance, kh√¥ng CV)**  
  - A1: BoW (1‚Äì2) + Multinomial Naive Bayes.  
  - A2: TF-IDF (1‚Äì2) + Logistic Regression.  
  - A3: TF-IDF (1‚Äì2) + Linear SVM.  

- **TF-IDF + Logistic Regression / Naive Bayes / SVM**  
  - Th·ª≠ v·ªõi unigram / bigram / trigram.  
  - So s√°nh hi·ªáu qu·∫£ gi·ªØa LR, NB v√† SVM.  

- **X·ª≠ l√Ω m·∫•t c√¢n b·∫±ng l·ªõp**  
  - D√πng `class_weight` cho Logistic Regression v√† Linear SVM.  
  - D√πng `sample_weight` cho Naive Bayes.  

- **Cross-validation v·ªõi GridSearchCV**  
  - TF-IDF bigram + LR / NB / SVM.  
  - BoW bigram + Logistic Regression (Attempt 15).  

### 3.3 K·∫øt qu·∫£ ch√≠nh (validation set)

B·∫£ng d∆∞·ªõi t√≥m t·∫Øt m·ªôt s·ªë th·ª≠ nghi·ªám ti√™u bi·ªÉu (ƒë·ªô ch√≠nh x√°c v√† F1-macro / F1-weighted):

| Th·ª≠ nghi·ªám | Accuracy | F1-macro | F1-weighted |
|-----------|----------|----------|-------------|
| A1 ‚Äì BoW (1‚Äì2) + Naive Bayes | 0.773 | 0.645 | 0.752 |
| A2 ‚Äì TF-IDF (1‚Äì2) + Logistic Regression | 0.808 | 0.720 | 0.795 |
| A3 ‚Äì TF-IDF (1‚Äì2) + Linear SVM | 0.897 | 0.863 | 0.896 |
| A6.2 ‚Äì TF-IDF bigram + SVM | 0.911 | 0.880 | 0.911 |
| A7.2 ‚Äì TF-IDF bigram, class_weight + LR | 0.907 | 0.881 | 0.909 |
| A9.2 ‚Äì TF-IDF bigram, class_weight + SVM | 0.911 | 0.881 | 0.911 |
| A15 ‚Äì BoW bigram, class_weight + LR (CV) | 0.908 | 0.880 | 0.909 |

**Nh·∫≠n x√©t nhanh**

- Khi chuy·ªÉn t·ª´ BoW ‚Üí TF-IDF + SVM (A3) th√¨ ƒë·ªô ch√≠nh x√°c tƒÉng m·∫°nh (~0.897) v√† F1-macro cao, m√¥ h√¨nh ph√¢n bi·ªát t·ªët h∆°n c√°c l·ªõp nh·ªè.  
- SVM v·ªõi TF-IDF bigram (A6.2) ƒë√£ ƒë·∫°t **Accuracy ‚âà 0.911**, F1-macro ‚âà 0.88.  
- Th√™m `class_weight` (A7.2, A9.2) gi√∫p c·∫£i thi·ªán c√¢n b·∫±ng gi·ªØa c√°c l·ªõp m√† kh√¥ng l√†m gi·∫£m hi·ªáu nƒÉng t·ªïng th·ªÉ; A9.2 ƒë·∫°t **Accuracy ‚âà 0.911, F1-macro ‚âà 0.881**.  
- BoW + LR v·ªõi `class_weight` v√† GridSearchCV (A15) c≈©ng cho k·∫øt qu·∫£ t·ªët (Accuracy ‚âà 0.908), nh∆∞ng TF-IDF + SVM v·∫´n nh·ªânh h∆°n.

**M√¥ h√¨nh truy·ªÅn th·ªëng ƒë∆∞·ª£c ch·ªçn**

- C·∫•u h√¨nh: **TF-IDF bigram + Linear SVM v·ªõi `class_weight` (Attempt 9.2)**.  
- L√Ω do: ƒë·∫°t Accuracy v√† F1-macro cao, ƒë·ªìng th·ªùi x·ª≠ l√Ω t·ªët m·∫•t c√¢n b·∫±ng l·ªõp.

## 4. Pipeline deep learning (CNN embedding)

Pipeline deep learning trong notebook g·ªìm c√°c b∆∞·ªõc:

1. **Tokenization & padding**  
   - S·ª≠ d·ª•ng Keras `Tokenizer` v·ªõi `max_words = 10 000`, `max_len = 100`.  
   - √Åp d·ª•ng chung cho train / val / test.

2. **Pretrained word embeddings**  
   - D√πng pretrained GloVe 300-d t·ª´ `glove-wiki-gigaword-300`.  
   - X√¢y d·ª±ng `embedding_matrix` cho t·ªëi ƒëa 10 000 t·ª´ ƒë·∫ßu.

3. **Ki·∫øn tr√∫c CNN**  
   - Embedding layer (fix ho·∫∑c fine-tune t√πy c·∫•u h√¨nh).  
   - 1 l·ªõp `Conv1D` (128 filters, kernel size = 5) + `GlobalMaxPooling1D`.  
   - Dense 64 neurons + `Dropout(0.5)`.  
   - Dense cu·ªëi: softmax v·ªõi 6 l·ªõp c·∫£m x√∫c.  
   - Hu·∫•n luy·ªán CNN trong **10 epochs** v·ªõi loss `sparse_categorical_crossentropy`.

4. **Tr√≠ch xu·∫•t embedding & Random Forest**  
   - D√πng output t·ª´ l·ªõp `Dropout` l√†m vector embedding c√¢u.  
   - Tr√≠ch xu·∫•t `X_train_embed`, `X_val_embed`, `X_test_embed`.  
   - Hu·∫•n luy·ªán **RandomForestClassifier** tr√™n c√°c embedding n√†y.

**K·∫øt qu·∫£ tr√™n test set (CNN embedding + Random Forest)**

- Accuracy: **0.89**  
- F1-macro: **0.84**  
- F1-weighted: **0.89**  

So v·ªõi m√¥ h√¨nh TF-IDF + Linear SVM, pipeline deep learning cho k·∫øt qu·∫£ t∆∞∆°ng ƒë∆∞∆°ng nh∆∞ng v·∫´n k√©m nh·∫π v·ªÅ Accuracy/F1-macro. B√π l·∫°i, embedding t·ª´ CNN c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng cho c√°c m√¥ h√¨nh ho·∫∑c task kh√°c.

## 5. K·∫øt lu·∫≠n & h∆∞·ªõng ph√°t tri·ªÉn

- EDA cho th·∫•y d·ªØ li·ªáu c·∫£m x√∫c:
  - M·∫•t c√¢n b·∫±ng gi·ªØa c√°c l·ªõp (ƒë·∫∑c bi·ªát l√† `surprise`).  
  - C√¢u t∆∞∆°ng ƒë·ªëi ng·∫Øn (trung b√¨nh ~19 t·ª´).  
- Pipeline truy·ªÅn th·ªëng v·ªõi **TF-IDF + Linear SVM (bigram, class_weight)** l√† m√¥ h√¨nh hi·ªáu qu·∫£ nh·∫•t trong lo·∫°t th·ª≠ nghi·ªám (Accuracy ‚âà 0.911, F1-macro ‚âà 0.881).  
- Pipeline deep learning v·ªõi CNN + GloVe + Random Forest cho k·∫øt qu·∫£ t·ªët (~0.89 accuracy) nh∆∞ng ch∆∞a v∆∞·ª£t SVM.

**H∆∞·ªõng m·ªü r·ªông**

- B·ªï sung b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω: lowercase, b·ªè stopword, lemmatization, x·ª≠ l√Ω emoji.  
- Th·ª≠ c√°c m√¥ h√¨nh embedding m·∫°nh h∆°n (BERT / RoBERTa, Sentence-Transformers).  
- T·ªëi ∆∞u th√™m hyperparameters b·∫±ng GridSearch/RandomSearch v√† cross-validation tr√™n to√†n b·ªô pipeline.  
- √Åp d·ª•ng k·ªπ thu·∫≠t x·ª≠ l√Ω m·∫•t c√¢n b·∫±ng kh√°c (SMOTE, focal loss).

---

üëâ To√†n b·ªô m√£ ngu·ªìn chi ti·∫øt n·∫±m trong notebook: **`CSML25_BTL2.ipynb`**.
